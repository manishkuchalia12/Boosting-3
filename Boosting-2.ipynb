{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb54390-8a8f-483e-b97e-0e09ff411780",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "Ans:-Gradient Boosting Regression is a machine learning technique used for regression tasks. It is an ensemble learning method that combines the predictions of multiple weak learners (typically decision trees) to create a strong predictive model. The key idea behind gradient boosting regression is to sequentially train weak learners, and each new learner corrects the errors made by the combined set of existing models.\r\n",
    "\r\n",
    "Here's a high-level overview of how Gradient Boosting Regression works:\r\n",
    "\r\n",
    "Initialization:\r\n",
    "\r\n",
    "The process starts with an initial model, which can be a simple one like the mean of the target variable.\r\n",
    "Sequential Training:\r\n",
    "\r\n",
    "A series of weak learners (often decision trees) are trained sequentially. Each new learner is trained on the residuals (the differences between the actual and predicted values) of the combined set of existing models.\r\n",
    "Gradient Descent Optimization:\r\n",
    "\r\n",
    "The new learner is trained to minimize the residual errors by using gradient descent optimization. The learning rate parameter controls the step size during optimization.\r\n",
    "Weighted Combination:\r\n",
    "\r\n",
    "The predictions of all weak learners are combined with weights, and each learner's weight is determined by its contribution to reducing the overall residual error. The combination is typically a weighted sum.\r\n",
    "Iterative Process:\r\n",
    "\r\n",
    "Steps 2-4 are repeated for a predefined number of iterations or until a specified condition is met. Each new learner focuses on correcting the errors of the combined set of existing models.\r\n",
    "Final Prediction:\r\n",
    "\r\n",
    "The final prediction is the sum of the initial model's prediction and the weighted contributions of all the sequentially trained weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9904e88-89a0-4bff-8a3d-3a1088b324cb",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "Ans:-\r\n",
    "Implementing a simple gradient boosting algorithm from scratch involves creating weak learners (usually decision trees), sequentially training them, and combining their predictions. Here's a basic example using Python and NumPy for a simple regression problem. We'll use a small dataset and evaluate the model's performance using mean squared error (MSE) and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9809e4f-2069-4289-933a-3705594fb76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X.squeeze() + np.random.randn(100)  # True relationship with some noise\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Gradient Boosting Regression class\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize with the mean of the target variable\n",
    "        initial_prediction = np.mean(y)\n",
    "        self.models.append((\"initial\", initial_prediction))\n",
    "\n",
    "        # Sequentially train weak learners\n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute residuals\n",
    "            residuals = y - self.predict(X)\n",
    "\n",
    "            # Train a weak learner (Decision Tree) on the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=3)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update the ensemble with the weak learner\n",
    "            self.models.append((\"tree\", tree))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using the ensemble of weak learners\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for model_type, model in self.models:\n",
    "            if model_type == \"initial\":\n",
    "                predictions += self.learning_rate * model\n",
    "            elif model_type == \"tree\":\n",
    "                predictions += self.learning_rate * model.predict(X)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# Instantiate and train the Gradient Boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Plot predictions\n",
    "plt.scatter(X_test, y_test, label=\"True values\", alpha=0.5)\n",
    "plt.scatter(X_test, y_pred, label=\"Predicted values\", alpha=0.5)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaebe0c-43cf-4fe0-83f9-a482f498e968",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n",
    "Ans:-\r\n",
    "Performing hyperparameter tuning is crucial for optimizing the performance of a gradient boosting model. Here, I'll demonstrate how to use scikit-learn's GridSearchCV to perform a grid search over different combinations of hyperparameters. In this example, we'll vary the learning rate, the number of trees (n_estimators), and the tree depth (max_depth). Please note that the actual hyperparameter search space might need further customization based on the specific characteristics of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c409341a-cec0-4874-a3cd-2986ebf16f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create the GradientBoostingRegressor and GridSearchCV objects\n",
    "gb_model = GradientBoostingRegressor()\n",
    "grid_search = GridSearchCV(gb_model, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Perform the grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_best = best_gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "# Print results\n",
    "print(\"Best Mean Squared Error:\", mse_best)\n",
    "print(\"Best R-squared:\", r2_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6017e10-54cb-492c-92a0-c4ce98a127fe",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "Ans:-In the context of gradient boosting, a weak learner refers to a model that performs slightly better than random chance on a binary classification task. Weak learners are also referred to as base learners or base models. In the case of regression tasks, weak learners are models that have predictive performance slightly better than predicting the mean of the target variable.\r\n",
    "\r\n",
    "The concept of weak learners is integral to gradient boosting algorithms, and they are typically decision trees with limited depth. Specifically, decision stumps (trees with a single split) or very shallow trees are commonly used as weak learners. The restriction on the complexity of weak learners is intentional and serves a key purpose in the gradient boosting process.\r\n",
    "\r\n",
    "Here are some characteristics of weak learners in the context of gradient boosting:\r\n",
    "\r\n",
    "Low Complexity:\r\n",
    "\r\n",
    "Weak learners are intentionally simple models with low complexity. They lack the capacity to capture complex patterns in the data.\r\n",
    "Slightly Better than Random:\r\n",
    "\r\n",
    "A weak learner should perform slightly better than random guessing. For binary classification, this means having an accuracy slightly above 50%, and for regression, it means having predictions that are slightly better than the mean of the target variable.\r\n",
    "Sequential Improvement:\r\n",
    "\r\n",
    "In the gradient boosting process, weak learners are trained sequentially. Each new weak learner focuses on correcting the errors made by the combined set of existing models.\r\n",
    "Contribution to Ensemble:\r\n",
    "\r\n",
    "Although individual weak learners may not perform well on their own, their combined predictions contribute to a strong ensemble model. The iterative nature of gradient boosting allows the ensemble to gradually improve its predictive performance.\r\n",
    "Interpretability:\r\n",
    "\r\n",
    "Weak learners are often chosen for their interpretability and simplicity. Shallow decision trees are easy to interpret, and their use facilitates the interpretability of the overall gradient boosting model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d8dc1e-b729-4bc1-9ca9-4be2ceffb0dd",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "Ans:-The Gradient Boosting algorithm is an ensemble learning technique that combines the predictions of multiple weak learners (often shallow decision trees) to create a strong predictive model. The intuition behind Gradient Boosting can be summarized as follows:\n",
    "\n",
    "Sequential Improvement:\n",
    "\n",
    "The algorithm starts with an initial prediction, which can be a simple one like the mean of the target variable for regression tasks or the log-odds for binary classification. Subsequent weak learners are then added sequentially to correct the errors made by the existing ensemble.\n",
    "Focus on Residuals:\n",
    "\n",
    "Each new weak learner is trained to predict the residuals (the differences between the actual and predicted values) of the current ensemble. This focuses the new learner on capturing the remaining patterns and errors in the data that the existing ensemble has not yet captured.\n",
    "Gradient Descent Optimization:\n",
    "\n",
    "The learning process involves using gradient descent optimization to find the parameters of the weak learner that minimize the loss function with respect to the residuals. The learning rate controls the step size during optimization.\n",
    "Combining Predictions:\n",
    "\n",
    "The predictions of all weak learners are combined, and each learner's contribution is weighted. The weights are determined by the learning rate and the performance of the weak learner in reducing the overall loss. The final prediction is the sum of these weighted contributions.\n",
    "Robustness to Overfitting:\n",
    "\n",
    "Gradient Boosting is less prone to overfitting compared to individual weak learners. The sequential addition of weak learners with a focus on correcting errors helps create a more generalized model.\n",
    "Adaptive Learning:\n",
    "\n",
    "The algorithm adapts over iterations, with each new learner placed more emphasis on instances that were difficult to predict for the existing ensemble. This adaptability contributes to the model's ability to handle complex relationships in the data.\n",
    "Flexibility with Loss Functions:\n",
    "\n",
    "Gradient Boosting is flexible in terms of loss functions, allowing it to be used for both regression and classification tasks. Common loss functions include mean squared error for regression and log loss for binary classification.\n",
    "Ensemble of Weak Models:\n",
    "\n",
    "The strength of Gradient Boosting lies in its ability to combine the predictions of many weak models to create a highly accurate and robust ensemble. The ensemble's complexity increases gradually with the addition of more weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659e283-4309-4512-9677-da93af202b5b",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "Ans:-The Gradient Boosting algorithm builds an ensemble of weak learners sequentially, with each new learner aiming to correct the errors made by the existing ensemble. The process involves the following steps:\r\n",
    "\r\n",
    "Initialization:\r\n",
    "\r\n",
    "Start with an initial prediction. For regression tasks, this might be the mean of the target variable, and for binary classification, it could be the log-odds.\r\n",
    "Compute Residuals:\r\n",
    "\r\n",
    "Calculate the residuals, which are the differences between the actual values and the current ensemble's predictions. These residuals represent the errors that the next weak learner should focus on correcting.\r\n",
    "Train Weak Learner:\r\n",
    "\r\n",
    "Train a weak learner (typically a shallow decision tree) on the residuals. The weak learner is trained to predict the residuals and is constrained in terms of its complexity, often with limitations on tree depth.\r\n",
    "Compute Learning Rate and Weight:\r\n",
    "\r\n",
    "Determine the learning rate, which controls the step size during optimization, and compute the weight assigned to the weak learner. The weight is based on the performance of the weak learner in reducing the overall loss.\r\n",
    "Update Ensemble:\r\n",
    "\r\n",
    "Update the ensemble by adding the weighted prediction of the new weak learner. The predictions of all weak learners in the ensemble are combined with their respective weights.\r\n",
    "Repeat Iteratively:\r\n",
    "\r\n",
    "Repeat steps 2-5 for a predefined number of iterations or until a stopping criterion is met. Each new weak learner is trained to focus on the residuals of the current ensemble, gradually improving the model's predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a15b288-2741-428d-b20a-a692a1fa7a2d",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
